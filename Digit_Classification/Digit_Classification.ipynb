{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Digit_Classification.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cPA1xIB00z81"
      },
      "source": [
        "Digit Classification Project with MNIST dataset\n",
        "The MNIST data set contains 60,000 training images and 10,000 test images."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pO4SVtMT_KaD"
      },
      "source": [
        "#Reference: \n",
        "# Â© MIT 6.S191: Introduction to Deep Learning\n",
        "# http://introtodeeplearning.com"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VcW5FEFs07qe"
      },
      "source": [
        "#Importing tensorflow alongwith other dependencies\n",
        "%tensorflow_version 2.x\n",
        "import tensorflow as tf\n",
        "\n",
        "!pip install mitdeeplearning\n",
        "import mitdeeplearning as mdl\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import random\n",
        "from tqdm import tqdm"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0WFuOARG2Fvw"
      },
      "source": [
        "#downloading and loading the dataset\n",
        "mnist = tf.keras.datasets.mnist\n",
        "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n",
        "train_images = (np.expand_dims(train_images, axis=-1)/255.).astype(np.float32)\n",
        "train_labels = (train_labels).astype(np.int64)\n",
        "test_images = (np.expand_dims(test_images, axis=-1)/255.).astype(np.float32)\n",
        "test_labels = (test_labels).astype(np.int64)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MpdPfnSn2aqP"
      },
      "source": [
        "#the dataset contains 28x28 grayscale  images of handwritten digits\n",
        "plt.figure(figsize=(10,10))\n",
        "random_inds = np.random.choice(60000,36)\n",
        "for i in range(36):\n",
        "    plt.subplot(6,6,i+1)\n",
        "    plt.xticks([])\n",
        "    plt.yticks([])\n",
        "    plt.grid(False)\n",
        "    image_ind = random_inds[i]\n",
        "    plt.imshow(np.squeeze(train_images[image_ind]), cmap=plt.cm.binary)\n",
        "    plt.xlabel(train_labels[image_ind])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "esfMIk7k3BGx"
      },
      "source": [
        "#defining simple neural network using keras API and sequential class\n",
        "\n",
        "def build_fc_model():\n",
        "  fc_model = tf.keras.Sequential([\n",
        "      # First define a Flatten layer\n",
        "      tf.keras.layers.Flatten(),\n",
        "\n",
        "      #Defining the activation function for the first fully connected (Dense) layer.'''\n",
        "      tf.keras.layers.Dense(128, activation=tf.nn.relu),\n",
        "      # tf.keras.layers.Dense(128, activation= '''TODO'''),\n",
        "\n",
        "      #Defining the second Dense layer to output the classification probabilities'''\n",
        "      tf.keras.layers.Dense(10, activation=tf.nn.softmax) \n",
        "      # [Dense layer to output classification probabilities]\n",
        "      \n",
        "  ])\n",
        "  return fc_model\n",
        "\n",
        "model = build_fc_model()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t7RlaC9yvx2p"
      },
      "source": [
        "#experimenting with both optimiter and learning rates\n",
        "model.compile(optimizer=tf.keras.optimizers.SGD(learning_rate=1e-1), \n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4BKW1Ro2zCY_"
      },
      "source": [
        "#defining the batch size and the number of epochs to use during training\n",
        "BATCH_SIZE = 64\n",
        "EPOCHS = 5\n",
        "\n",
        "model.fit(train_images, train_labels, batch_size=BATCH_SIZE, epochs=EPOCHS)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "54euWTlSzdDD"
      },
      "source": [
        "#testing the model with evaluate method\n",
        "test_loss, test_acc = model.evaluate(test_images, test_labels)\n",
        "\n",
        "print('Test accuracy:', test_acc)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UkfoT1U00CuM"
      },
      "source": [
        "#defining a cnn model and will use same training and test dataset to get more accuracy\n",
        "def build_cnn_model():\n",
        "    cnn_model = tf.keras.Sequential([\n",
        "\n",
        "        #Defining the first convolutional layer\n",
        "        tf.keras.layers.Conv2D(filters=24, kernel_size=(3,3), activation=tf.nn.relu),\n",
        "        # tf.keras.layers.Conv2D('''TODO''') \n",
        "\n",
        "        #Defining the first max pooling layer\n",
        "        tf.keras.layers.MaxPool2D(pool_size=(2,2)),\n",
        "        # tf.keras.layers.MaxPool2D('''TODO''')\n",
        "\n",
        "        #Definingthe second convolutional layer\n",
        "        tf.keras.layers.Conv2D(filters=36, kernel_size=(3,3), activation=tf.nn.relu),\n",
        "        # tf.keras.layers.Conv2D('''TODO''')\n",
        "\n",
        "        #Defining the second max pooling layer\n",
        "        tf.keras.layers.MaxPool2D(pool_size=(2,2)),\n",
        "        # tf.keras.layers.MaxPool2D('''TODO''')\n",
        "\n",
        "        tf.keras.layers.Flatten(),\n",
        "        tf.keras.layers.Dense(128, activation=tf.nn.relu),\n",
        "\n",
        "        #Defining the last Dense layer to output the classification \n",
        "        # probabilities. Pay attention to the activation needed a probability\n",
        "        # output\n",
        "        tf.keras.layers.Dense(10, activation=tf.nn.softmax)\n",
        "        # [Dense layer to output classification probabilities]\n",
        "    ])\n",
        "    \n",
        "    return cnn_model\n",
        "  \n",
        "cnn_model = build_cnn_model()\n",
        "# Initializing the model by passing some data through\n",
        "cnn_model.predict(train_images[[0]])\n",
        "# Print the summary of the layers in the model.\n",
        "print(cnn_model.summary())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nZCB-trb8Y_f"
      },
      "source": [
        "#Defining the compile operation with your optimizer and learning rate of choice\n",
        "cnn_model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3),\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w_FKKbEH8loz"
      },
      "source": [
        "#Using model.fit to train the CNN model, with the same batch_size and number of epochs previously used\n",
        "cnn_model.fit(train_images, train_labels, batch_size=BATCH_SIZE, epochs=EPOCHS)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1vzlUeQv8xx3"
      },
      "source": [
        "#Using the evaluate method to test the model!\n",
        "test_loss, test_acc = cnn_model.evaluate(test_images, test_labels)\n",
        "\n",
        "print('Test accuracy:', test_acc)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xKae1M6u87MA"
      },
      "source": [
        "#making prediction with cnn model\n",
        "predictions = cnn_model.predict(test_images)\n",
        "predictions[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kkYED3Ko9EgD"
      },
      "source": [
        "#identify the digit with the highest confidence prediction for the first image in the test dataset.\n",
        "prediction = np.argmax(predictions[0]) \n",
        "\n",
        "print(prediction)\n",
        "\n",
        "print(\"Label of this digit is:\", test_labels[0])\n",
        "plt.imshow(test_images[0,:,:,0], cmap=plt.cm.binary)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fnH7Pz9E9X8V"
      },
      "source": [
        "#plot images from the test dataset along with their predicted label,\n",
        "# as well as a histogram that provides the prediction probabilities for each of the digits\n",
        "image_index = 81 #@param {type:\"slider\", min:0, max:100, step:1}\n",
        "plt.subplot(1,2,1)\n",
        "mdl.lab2.plot_image_prediction(image_index, predictions, test_labels, test_images)\n",
        "plt.subplot(1,2,2)\n",
        "mdl.lab2.plot_value_prediction(image_index, predictions,  test_labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DraCjaKy9rRs"
      },
      "source": [
        "# Plots the first X test images, their predicted label, and the true label\n",
        "# Color correct predictions in blue, incorrect predictions in red\n",
        "num_rows = 5\n",
        "num_cols = 4\n",
        "num_images = num_rows*num_cols\n",
        "plt.figure(figsize=(2*2*num_cols, 2*num_rows))\n",
        "for i in range(num_images):\n",
        "  plt.subplot(num_rows, 2*num_cols, 2*i+1)\n",
        "  mdl.lab2.plot_image_prediction(i, predictions, test_labels, test_images)\n",
        "  plt.subplot(num_rows, 2*num_cols, 2*i+2)\n",
        "  mdl.lab2.plot_value_prediction(i, predictions, test_labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E_Q3uKajFK0b"
      },
      "source": [
        "Reference\n",
        "# Â© MIT 6.S191: Introduction to Deep Learning\n",
        "# http://introtodeeplearning.com"
      ]
    }
  ]
}